%\VignetteIndexEntry{faoswsAupus: A package replicating the logic of the AUPUS
% statistical working system}
%\VignetteEngine{knitr::knitr}
\documentclass[nojss]{jss}
\usepackage{url}
\usepackage[sc]{mathpazo}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{breakurl}
\usepackage{hyperref}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{mathtools}
\usepackage{draftwatermark}
\usepackage{float}
\usepackage{placeins}
\usepackage{mathrsfs}
\usepackage{multirow}
%% \usepackage{mathbbm}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\title{\bf faoswsAupus: A package replicating the logic of the AUPUS
statistical working system}

\author{Joshua M. Browning, Michael. C. J. Kao\\ Food and Agriculture
    Organization \\ of the United Nations\\}

\Plainauthor{Joshua M. Browning, Michael. C. J. Kao}

\Plaintitle{faoswsAupus: A package replicating the logic of the AUPUS
statistical working system}

\Shorttitle{AUPUS Module}

\Abstract{ 

  This vignette provides a detailed description of the usage of
  functions in the \pkg{faoswsAupus} package. \\
  
}

\Keywords{AUPUS, Standardization}
\Plainkeywords{AUPUS, Standardization}

\Address{
  Joshua M. Browning and Michael. C. J. Kao\\
  Economics and Social Statistics Division (ESS)\\
  Economic and Social Development Department (ES)\\
  Food and Agriculture Organization of the United Nations (FAO)\\
  Viale delle Terme di Caracalla 00153 Rome, Italy\\
  E-mail: \email{joshua.browning@fao.org, michael.kao@fao.org}\\
  URL: \url{https://svn.fao.org/projects/SWS/RModules/faoswsAupus/}
}

\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
opts_chunk$set(fig.path='figure/', fig.align='center', fig.show='hold',
               warning=FALSE, message=FALSE, error=FALSE, tidy=FALSE, 
               results='markup', eval=TRUE, echo=TRUE, cache=FALSE, dpi=200)
options(replace.assign=TRUE, width=80)
assign("depthtrigger", 10, data.table:::.global)
@ 

\section{Introduction}

First, let us give an overview of the procedure for creating the food balance sheets.  The data we collect for food balance sheets is usually at the primary level (e.g. wheat, milk, etc.).  However, there are several exceptions: first, some countries may report production of non-primary products (flour, beer, etc.) but this is not commonly the case.  Additionally, trade information is provided in great detail: we may know how much bread went from country A to country B.  We must account for these trade imbalances within the commodity balances (for example, we must consider the trade imbalance of bread in the wheat commodity tree).  One approach would be to simply roll-up all trade imbalances into the top level equivalent (i.e. wheat, in this example) but there are further complications.  Processing of wheat into flour creates bran and germ, and these products are almost entirely put into feed utilization.

Thus, we take the following approach in creating the food balance sheets.

\begin{enumerate}
    \item We start with the ``primary'' commodities: wheat, barley, milk, etc.
    \begin{enumerate}
        \item If some elements of the balance are missing (i.e. our modules have failed to impute them), then impute these values using the old AUPUS methodology (as a ratio of production, total supply, etc.).
        \item Now, we must balance at this level.  Balancing is done by maximum likelihood or something similar, and so measurement error is allocated to all products (with more error allocated to products with larger estimated variances).
    \end{enumerate}
    \item Now, the amount allocated to food for the primary commodity is to be converted into production at the first processed level (flour, butter, etc.) if the values at that level are missing (otherwise the official data is used).  Note: if official data is available at this level, this should inform the shares or the food from the primary commodity but should not be used to compute the extraction rate.  \textbf{How does this drive what we do on the primary level?  If this is the only child, then we just update the food for the primary.  But, if there are multiple children (some without official data) then does this knowledge inform an adjustment to the food or the shares or both?}
    \begin{enumerate}
        \item First, some of the primary commodity may be eaten as such, and hence the percent going to processing should be determined and a quantity then removed from food going to processing.
        \item Next, the shares (which specifies how the processed commodity should be allocated to all of it's available children) is applied to allocate the amount processed into it's various children.
        \item Lastly,  the extraction rate (which specifies a sort of conversion rate from a parent commodity to it's child) should be applied to convert the processed parent commodity into quantity of the child.
    \end{enumerate}
    For example, suppose we want to create flour production from wheat.  We may have 90\% of wheat being processed (and 10\% left as such), 95\% of processed wheat that is allocated to flour and 5\% allocated to beer, and an 80\% extraction rate of wheat to flour.  Then, if we had 100 thousand tons of wheat, we would convert this to 100(90\%)(95\%)(80\%) = 68.4 thousand tons of flour.
    \item In the above step, we must also be careful to create all appropriate elements as specified by the tree structure.  For example, bran and germ are also created when wheat is processed into flour.  These are not elements with separate shares but should be thought of as by-products in the creation of flour.
    \item We continue steps 1-3 and process commodities down the commodity tree and balance at each step until we reach a point where no further processing is required.  For example, wheat must be processed down into flour so we can create bran and germ (as these elements often get allocated to feed).  However, it does not need to be processed into bread, as no important information is gained in that process.  Another interesting example is the barley tree: barley must be processed into barley malt and then barley beer, and this processing must occur because alcoholic products are placed in a different group in the standardization (i.e. aggregation) of the commodity tree.
    \item We can now compute nutrient information (calories, proteins, fats) from quantities.
    \item Now, we must standardize everything back up to it's primary equivalent.  We may have trade data at lower levels of processing as well, and these must now move back up the commodity trees.  Thus, we start at the bottom nodes and divide by extraction rates to compute parent quantites.  Calories, on the other hand, can be added directly in the standardization process.  \textbf{Is this right???}  However, there are several special cases/important notes:
    \begin{itemize}
        \item With ``by-products'' (for example, wheat bran and germ) we do not standardize quantities as they are already accounted for in the main product standardization.  However, standardization of calories/fats/proteins is performed for all products by adding the calorie/fat/protein values.
        \item Some products (oils, juices, beers, etc.) can be created from multiple parents.  In this case, the products must be rolled up into various parents, and the appropriate allocation to parents is not clear.  We may use shares to determine this allocation, but we could have problems if a country has a large trade deficit in a child and little availability in a parent (or even larger problems if default shares are used and a country does not actually produce or trade a parent).  Thus, allocation should be generally done based on availability.  However, in some cases we need to be able to specify that preference be given to certain parents.  An example of this could be beer where preference should be given to barley over, say, bananas, wheat, etc.
        \item Separate trees may be used for processing vs. standardizing, as some commodities do not roll up into their respective parents (e.g. beer is not in cereals and butter is not in milk).
        \item Production should \textbf{not} be standardized.  This is because production of children commodities come directly from food of parent commodities, and so essentially they are already accounted for.  All other elements should be standardized, though.  Alternatively, we could standardize production if we deduct input from processing values, but that seems to add complication to the standardization procedure and it is not clear if this will improve our estimates.
    \end{itemize}
\end{enumerate}

\section{Data Initialization}

First, we need to load the relevant packages:

<<load-packages>>=
library(faoswsAupus)
library(faosws)
library(igraph)
library(data.table)
@

This package comes with many functions for replicating the AUPUS and 
standardization procedures as well as several useful datasets.  The first we
will use is called 'US', and it contains an example of the data that is used
within this package.  There is also a variable called usAupusParam, and that
variable contains information about the US dataset.

<<>>=
is(US)
names(US)
sapply(US, class)
sapply(US, dim)
is(usAupusParam)
names(usAupusParam)
@

Note: usually these values would be generated in the following manner:

<<eval=FALSE>>=
GetTestEnvironment(
    baseUrl = "https://hqlprswsas1.hq.un.fao.org:8181/sws",
    token = "34c18199-4f48-4ce4-a8d5-16ce62e8e23d")
usAupusParam = getAupusParameter(areaCode = "231", assignGlobal = FALSE,
                                 yearsToUse = 2009:2013)
US = getAupusDataset(aupusParam = usAupusParam)
@

The GetTestEnvironment function from the faosws package sets up the right
variables in R for querying the SWS database, and the specific token I used
provides the necessary information for the AUPUS dataset.  The parameters and
data are then pulled from the SWS.  However, for this vignette, we'll just use
the data that already exists in this package.

We see that the US dataset contains 8 data.tables.  Let's look at a subset of
this data to more easily understand what we're working with.  First, let's
ensure we grab a meaningful subset of data.  The plotCommodityTree function
takes the shareData dataset and generates a plot for how the commodities are
related to one another.

<<>>=
plotCommodityTree(US$shareData)
@

For simplicity, let's look at only commodities 71, 72, and 73.

<<>>=
US = subsetAupus(aupusData = US, parentKeys = c(71, 72, 73),
                 aupusParam = usAupusParam)
@

Now, we wish to work with this data in a network framework, and the package
contains a few functions to generate that framework:

<<>>=
aupusNetwork = suaToNetworkRepresentation(dataList = US,
                                          aupusParam = usAupusParam)
names(aupusNetwork)
sapply(aupusNetwork, class)
sapply(aupusNetwork, dim)
@

We see that now the data has been condensed down into two objects: nodes and
edges.  The nodes dataset has 15 rows (5 years times 3 commodities).
There are only 10 rows in the edges dataset, as there are only 2 edges times
5 years.  The nodes dataset is essentially the merged aupusData, itemInfoData,
ratioData, balanceElementData, and populationData from US, while the edges
dataset contains the datasets shareData, extractionRateData, and inputData from
US.  Here's our reduced network visualization:

<<>>=
plotCommodityTree(US$shareData, edge.arrow.size = 2, vertex.size = 25)
aupusNetwork$nodes[, .(geographicAreaFS, timePointYearsSP, measuredItemFS)]
colnames(aupusNetwork$nodes)[1:10]
aupusNetwork$edges[, .(geographicAreaFS, timePointYearsSP,
                       measuredItemParentFS, measuredItemChildFS)]
colnames(aupusNetwork$edges)
@

\section{AUPUS}

The entire AUPUS procedure is encapsulated in one function: Aupus.  However,
let's look at some of the functions called within this main function to
understand how the process works.  First, we have to set up some of the things
as done by the Aupus function:

<<>>=
nodes = aupusNetwork$nodes
edges = aupusNetwork$edges
nodes = coerceColumnTypes(aupusParam = usAupusParam, data = nodes)
edges = coerceColumnTypes(aupusParam = usAupusParam, data = edges)
from = usAupusParam$keyNames$itemParentName
to = usAupusParam$keyNames$itemChildName
processingLevelData = edges[, findProcessingLevel(.SD, from = from, 
    to = to, aupusParam = usAupusParam),
    by = c(usAupusParam$keyNames$areaName, usAupusParam$keyNames$yearName)]
setkeyv(processingLevelData, key(nodes))
invisible(nodes[processingLevelData, `:=`(processingLevel, i.processingLevel)])
invisible(nodes[is.na(processingLevel), processingLevel := 0])
nodes[, c(key(nodes), "processingLevel"), with = FALSE]
@

The processing level function above uses functions from the \pkg{igraph}
package to determine the ``processing level.''  This value indicates the order
in which a particular node is processed: nodes at level 0 have no
inputs/dependencies/children, and thus they can be processed immediately.  Once
level 0 nodes have been processed, we have the data necessary for processing
their parents, and these are nodes with processing level 1.  Aggregation
continues until all processing levels have been processed.

Our example is simple: 72 and 73 are children of 71, and so 71 must first be
processed before we process 72 and 73.  Thus, 71 is processingLevel = 0 and
72 and 73 have processingLevel = 1.

At each level, there are three main processes that are performed:
\begin{enumerate}
    \item The main AUPUS module is ran on each node.  This module computes each
    individual element following the logic of the old system.
    \item The edges of the graph are updated.
    \item The inputs from processing are updated.
\end{enumerate}

\subsection{Main AUPUS Module}

The main function here is calculateAupusElements.  This function calls all of
the individual element calculation functions.  Each element function has it's
own calculation, and is documented within it's help page.  However, we'll show
an example for a few of the functions.  First, though, note that we must subset
the AUPUS data: we only want to process the commodities at the lowest
processing level right now:

<<>>=
toProcess = nodes[processingLevel == 0, ]
@

Ok, now let's compute element 11:

<<>>=
toProcess[, Value_measuredElementFS_11]
toProcess[, Value_measuredElementFS_161]
toProcess[, flagFaostat_measuredElementFS_11]
calculateEle11(data = toProcess, aupusParam = usAupusParam)
toProcess[, Value_measuredElementFS_11]
toProcess[, flagFaostat_measuredElementFS_11]
@

Element 11 is called ``Initial Existence'' and thus it is set to
``Final Existence'' (element 161) from the previous year, if that value exists
and if element 11 is currently missing.  In this case, all of element 161's
values are missing and thus no updating occurs.  Let's continue processing some
elements:

<<>>=
calculateEle21(data = toProcess, aupusParam = usAupusParam)
calculateEle41(data = toProcess, aupusParam = usAupusParam)
calculateEle51(data = toProcess, aupusParam = usAupusParam)
calculateEle314151(data = toProcess, aupusParam = usAupusParam)
calculateEle63(data = toProcess, aupusParam = usAupusParam)
calculateEle71(data = toProcess, aupusParam = usAupusParam)
calculateEle93(data = toProcess, aupusParam = usAupusParam)
calculateTotalSupply(data = toProcess, aupusParam = usAupusParam)
tail(colnames(toProcess))
toProcess$TOTAL_SUPPLY
@

Each of the calculateEleXX functions above calculates updated values for each
element and returns a vector of list of vectors with indices.  These indices
represent the rows that were updated in the computation of this element.  The
last function, calculateTotalSupply, returns nothing but adds an additional
column to the toProcess data.table.  This column, TOTAL\_SUPPLY, represents the
total supply for this commodity.  Now, we can proceed with computing other
elements.  Some elements, such as 101, 111, 121, 131, and 141 use total supply
to fill in their values:

<<>>=
calculateEle101(stotal = "TOTAL_SUPPLY", data = toProcess,
                aupusParam = usAupusParam)
calculateEle111(stotal = "TOTAL_SUPPLY", data = toProcess,
                aupusParam = usAupusParam)
calculateEle121(stotal = "TOTAL_SUPPLY", data = toProcess,
                aupusParam = usAupusParam)
calculateEle131(stotal = "TOTAL_SUPPLY", data = toProcess,
                aupusParam = usAupusParam)
calculateEle141(stotal = "TOTAL_SUPPLY", data = toProcess,
                aupusParam = usAupusParam)
calculateEle144(population11Num = "Value_population_11",
                data = toProcess, aupusParam = usAupusParam)
calculateEle151(stotal = "TOTAL_SUPPLY", data = toProcess,
                aupusParam = usAupusParam)
calculateEle161(data = toProcess, aupusParam = usAupusParam)
calculateEle171(data = toProcess, aupusParam = usAupusParam)
calculateEle174(population11Num = "Value_population_11",
                                   data = toProcess,
                                   aupusParam = usAupusParam)
@

Once we've computed element 141, we can compute nutritive values: calories,
proteins, and fats.  These are based on ratios provided in the database (to
convert quantity into these values).

<<>>=
calculateTotalNutritive(ratioNum = "Ratio_measuredElementFS_261",
                            elementNum = 261, data = toProcess,
                            aupusParam = usAupusParam)
calculateDailyNutritive(population11Num = "Value_population_11",
                        population21Num = "Value_population_21",
                        dailyElement = 264, totalElement = 261,
                        data = toProcess, aupusParam = usAupusParam)
calculateTotalNutritive(ratioNum = "Ratio_measuredElementFS_271",
                            elementNum = 271, data = toProcess,
                            aupusParam = usAupusParam)
calculateDailyNutritive(population11Num = "Value_population_11",
                        population21Num = "Value_population_21",
                        dailyElement = 274, totalElement = 271,
                        data = toProcess, aupusParam = usAupusParam)
calculateTotalNutritive(ratioNum = "Ratio_measuredElementFS_281",
                            elementNum = 281, data = toProcess,
                            aupusParam = usAupusParam)
calculateDailyNutritive(population11Num = "Value_population_11",
                        population21Num = "Value_population_21",
                        dailyElement = 284, totalElement = 281,
                        data = toProcess, aupusParam = usAupusParam)
@

We now need to calculate two remaining elements (541 and 546, containing final
and total demand) and then we can balance.

<<>>=
calculateEle541(data = toProcess, aupusParam = usAupusParam)
calculateEle546(data = toProcess, aupusParam = usAupusParam)
calculateTotalUtilization(data = toProcess, aupusParam = usAupusParam)
calculateBalance(supply = "TOTAL_SUPPLY", utilization = "TOTAL_UTILIZATION",
                 data = toProcess, aupusParam = usAupusParam)
tail(colnames(toProcess))
@

Now, we have the TOTAL\_SUPPLY, TOTAL\_UTILIZATION, and BALANCE values.

\subsection{Update Edges}

The second step in the AUPUS procedure is to update the edges of the commodity
network.  In this step, we're updating the extraction rates and the input from
processing values on the edges data.table with the values from the nodes
table.  Note: the 131 element represents the input from processing value, and
the 41 element has extraction rates.

<<>>=
toProcess[, c("timePointYearsSP", "Value_measuredElementFS_131",
              "Value_measuredElementFS_41"), with = F]
edges[, c("timePointYearsSP", "measuredItemChildFS", "Value_share",
          "Value_input", "Value_extraction"), with = FALSE]
updateEdges(nodes = toProcess, edges = edges, aupusParam = usAupusParam)
edges[, c("timePointYearsSP", "measuredItemChildFS", "Value_share",
          "Value_input", "Value_extraction"), with = FALSE]
@

The Value\_input column of edges is updated (in theory) to reflect the amount
of the parent commodity flowing to the child.  In this case, the original
values for inputs from processing were already correct, and so nothing is
changed with them.  However, the extraction rates are different, and those
values are changed on the edges table (updated to reflect the nodes table).

\subsection{Update Inputs from Processing}

In this step, the data from the edges data.table is passed to the nodes
data.table at the next processing level.

<<>>=
nodesNextLevel = nodes[processingLevel == 1, ]
nodesNextLevel[, c("timePointYearsSP", "measuredItemFS",
                   "Value_measuredElementFS_31"), with = FALSE]
updateInputFromProcessing(nodes = nodesNextLevel,
                          edges = edges,
                          aupusParam = usAupusParam)
nodesNextLevel[, c("timePointYearsSP", "measuredItemFS",
                   "Value_measuredElementFS_31"), with = FALSE]
@

This entire section (i.e. the whole AUPUS procedure) can be run by calling
the function Aupus.  This function will also compute the processing levels and
iterate through each of them.

\section{Standardization}

Standardization refers to the process of aggregating multiple commodities up
to one representative commodity.  For example, wheat can be reported
directly, or derivatives of wheat, such as wheat flour, can also be reported.
To get a simpler view of the food balance sheet, we can ``standardize''
commodities up to their parent commodities.  This allows us to reduce the size
of the food balance sheet and make it easier to understand/analyze, but still
retains all of the food information available.

Now, from the previous sections, we have an object called
``updatedAupusNetwork'' which contains a data.table called ``nodes'' and a
data.table called ``edges.''  We can convert this object into a graph object
using the constructStandardizationGraph function.  We also pass a character
vector containing the names of the FBS element variables we're interested in.

<<>>=
FBSelements =
    c("Value_measuredElementFS_51", "Value_measuredElementFS_61",
      "Value_measuredElementFS_91", "Value_measuredElementFS_101",
      "Value_measuredElementFS_111", "Value_measuredElementFS_121",
      "Value_measuredElementFS_141", "Value_measuredElementFS_151")
standardizationGraph = 
    constructStandardizationGraph(aupusNetwork = aupusNetwork,
                                  standardizeElement = FBSelements,
                                  aupusParam = usAupusParam)
is(standardizationGraph)
sapply(standardizationGraph, class)
@

Let's now begin the standardization procedure:

<<>>=
standardization(standardizationGraph[[1]], standardizeElement = FBSelements,
                plot = TRUE, aupusParam = usAupusParam,
                vertex.size = 20, edge.arrow.size = 2, vertex.label.cex = 1)
@

To understand more clearly what happened, let's examine element 51 (which
represents the quantity of produced goods).

<<>>=
vertex.attributes(standardizationGraph[[1]])[1:2]
edge.attributes(standardizationGraph[[1]])[c(3, 4)]
E(standardizationGraph[[1]])
@

So, items 72 and 73 will be standardized to item 71 (based on the edges).  The
current value for element 51, item 71 is 177,630.  All 67,040 units from item
72 can be standardized/converted to item 71.  This is because the Value\_share
is 100, implying 100\% of item 72 came from 71 (and not other commodities as
well).  The extraction rate values presented are out of a base of 10,000, so
for 71 to 72 the actual extraction rate is 8,000/10,000 = 80\%.  Thus, we can
manually calculate the value for element 51, item 71:

<<>>=
177630 +                           # The initial value
    67040 * 10000/8000 * 100/100 + # Standardizing element 72
    13408 * 10000/1600 * 100/100   # Standardizing element 73
@

This value matches what we computed using the standardization function above.
The standardization function, however, performs the above operations for each
commodity and element type, and it performs the procedure multiple times (if
necessary) to standardize multiple levels back to one main commodity.  The
fbsStandardization function performs this standardization for each graph and
returns the result as a data.table.

<<>>=
fbsStandardization(graph = standardizationGraph, 
                   standardizeElement = FBSelements,
                   plot = FALSE, aupusParam = usAupusParam)
@

\end{document}